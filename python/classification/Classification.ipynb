{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "\n",
    "# Pre-processing\n",
    "from lib._class.DFDuplicateRemoval import DFDuplicateRemoval\n",
    "\n",
    "# Feature scaling\n",
    "from lib._class.DFRobustScaler import DFRobustScaler\n",
    "from lib._class.DFMinMaxScaler import DFMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_GRAPH   = 'resources/output/graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Loading\n",
    "- Reference: https://www.kaggle.com/mlg-ulb/creditcardfraud/home\n",
    "- Time: Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
    "- V1-V28: May be result of a PCA dimensionality reduction to protect user identities and sensitive features\n",
    "- Amount: Transaction amount\n",
    "- Class: 1 for fraudulent transactions, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.read_csv(f'{SOURCE_PATH_DATA}creditcard.csv', sep=',', chunksize=50_000,\n",
    "                        dtype={'Class': str},\n",
    "                        nrows=None)\n",
    "data_df   = pd.concat(df_chunks)\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.histogram(data_df,\n",
    "             bin_algo='count',\n",
    "             max_col=4,\n",
    "             title='Phase 1 - Histogram',\n",
    "             out_path=OUT_PATH_GRAPH,\n",
    "             layout_kwargs={'height': 2048})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.box(data_df,\n",
    "       color='Class',\n",
    "       max_col=4,\n",
    "       title='Phase 1 - Box',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'height': 2048,\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.kde(data_df,\n",
    "       color='Class',\n",
    "       max_col=4,\n",
    "       title='Phase 1 - KDE',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'height': 2048,\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Feature Engineering\n",
    "- Time & Amount features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.kaggle.com/miguelniblock/optimizing-imbalanced-classification-100-recall\n",
    "data_df['Hour'] = pd.to_timedelta(data_df['Time'], unit='s').dt.components.hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_df, x='Hour', facet_col='Class')\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename='Phase 2 - Histogram - Hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_df, x='Time', facet_col='Class')\n",
    "vp.generate_plot(fig,\n",
    "                 out_path=OUT_PATH_GRAPH,\n",
    "                 out_filename='Phase 2 - Histogram - Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.box(data_df[['Time', 'Hour', 'Amount', 'Class']],\n",
    "       color='Class',\n",
    "       max_col=2,\n",
    "       title='Phase 2 - Box',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.kde(data_df[['Time', 'Hour', 'Amount', 'Class']],\n",
    "       color='Class',\n",
    "       max_col=2,\n",
    "       title='Phase 2 - KDE',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less non-fraud transaction\n",
    "data_df['LTE_4_Hour'] = np.where(data_df['Hour'] <= 4, 1,0)\n",
    "\n",
    "# Normal distribution time\n",
    "data_df['LTE_100K_Time'] = np.where(data_df['Time'] <= 100_000, 1,0)\n",
    "\n",
    "# Data type conversion\n",
    "data_df['Class'] = data_df['Class'].astype(int)\n",
    "data_df = pd.concat([\n",
    "    data_df[[x for x in data_df.columns if x != 'Class']],\n",
    "    data_df['Class']\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.corrmat(data_df,\n",
    "           absolute=True,\n",
    "           matrix_type='upper',\n",
    "           title='Phase 2 - Correlation Matrix',\n",
    "           out_path=OUT_PATH_GRAPH,\n",
    "           heatmap_kwargs={\n",
    "               'reversescale': True\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Data Preparation\n",
    "- Remove duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_removal = DFDuplicateRemoval(target='Class')\n",
    "duplicate_removal.fit(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe duplicated data\n",
    "duplicate_df = duplicate_removal.duplicate_df\n",
    "\n",
    "duplicate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(duplicate_df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe if duplicated data are having different target label\n",
    "duplicate_df.groupby(duplicate_removal.subset).agg(\n",
    "    Class=('Class', 'mean')\n",
    ").reset_index()['Class'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated data by keeping 1st record\n",
    "data_df = duplicate_removal.transform(data_df)\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Classification\n",
    "- Separate features & target\n",
    "- Separate dataset\n",
    "- Feature scaling\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X = data_df[[x for x in data_df.columns if x != 'Class']]\n",
    "y = data_df['Class']\n",
    "\n",
    "vp.value_count(y.to_frame(), 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(X, y, random_state=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_state)\n",
    "    \n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    X_test  = X_test.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    y_test  = y_test.reset_index(drop=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset\n",
    "X_train, X_test, y_train, y_test = dataset_split(X, y, random_state=0)\n",
    "\n",
    "print('Train dataset:')\n",
    "vp.value_count(y_train.to_frame(), 'Class')\n",
    "print('\\nTest dataset:')\n",
    "vp.value_count(y_test.to_frame(), 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "robust_scaler = DFRobustScaler(columns=['Time', 'Amount', 'Hour'])\n",
    "minmax_scaler = DFMinMaxScaler()\n",
    "\n",
    "steps = [\n",
    "    ('robust_scaler', robust_scaler),\n",
    "    ('minmax_scaler', minmax_scaler),\n",
    "]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "X_train = pipeline.fit_transform(X_train)\n",
    "X_test  = pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_classif(X, y, model):\n",
    "    y_pred = model.predict(X)\n",
    "    y_true = y\n",
    "    \n",
    "    cofmat_df = pd.DataFrame(confusion_matrix(y_true, y_pred))\n",
    "    cofmat_df.index.name   = 'True'\n",
    "    cofmat_df.columns.name = 'Pred'\n",
    "\n",
    "    print(cofmat_df)\n",
    "    print()\n",
    "    print(classification_report(y_true, y_pred, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "eval_classif(X_train, y_train, model)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "eval_classif(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes           = np.unique(y_train)\n",
    "weights           = compute_class_weight('balanced', classes, y_train)\n",
    "class_weight_dict = {classes[i]: x for i,x in enumerate(weights)}\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=0, class_weight=class_weight_dict)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "eval_classif(X_train, y_train, model)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "eval_classif(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Ratio Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# - https://machinelearningmastery.com/cost-sensitive-neural-network-for-imbalanced-classification/?fbclid=IwAR1PcEicqDXadG9hsNE-Tf4RQQ_DpIaCV4LRcuizGbTC9Ek5PiMbB_x26bU\n",
    "# - https://www.youtube.com/watch?v=D6AChZlN5m0\n",
    "n_class0          = y_train.value_counts().loc[0]\n",
    "n_class1          = y_train.value_counts().loc[1]\n",
    "class_weight_dict = {0: 1, 1: n_class0 / n_class1}\n",
    "\n",
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=0, class_weight=class_weight_dict)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train dataset:')\n",
    "eval_classif(X_train, y_train, model)\n",
    "\n",
    "print('\\nTest dataset:')\n",
    "eval_classif(X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
