{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "import lib._util.mlpipe as mlpipe\n",
    "\n",
    "# Pre-processing\n",
    "from lib._class.DFDuplicateRemoval import DFDuplicateRemoval\n",
    "\n",
    "# Feature scaling\n",
    "from lib._class.DFStandardScaler import DFStandardScaler\n",
    "from lib._class.DFMinMaxScaler import DFMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced-Learn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_GRAPH   = 'resources/output/graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Loading\n",
    "- Reference: https://www.kaggle.com/mlg-ulb/creditcardfraud/home\n",
    "- Time: Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
    "- V1-V28: May be result of a PCA dimensionality reduction to protect user identities and sensitive features\n",
    "- Amount: Transaction amount\n",
    "- Class: 1 for fraudulent transactions, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.read_csv(f'{SOURCE_PATH_DATA}creditcard.csv', sep=',', chunksize=50_000)\n",
    "data_df   = pd.concat(df_chunks)\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(data_df, 'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.histogram(data_df,\n",
    "             bin_algo='count',\n",
    "             max_col=4,\n",
    "             title='Phase 1 - Histogram',\n",
    "             out_path=OUT_PATH_GRAPH,\n",
    "             layout_kwargs={'height': 2048})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.box(data_df,\n",
    "       color='Class',\n",
    "       max_col=4,\n",
    "       title='Phase 1 - Box',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'height': 2048,\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.kde(data_df,\n",
    "       color='Class',\n",
    "       max_col=4,\n",
    "       title='Phase 1 - KDE',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'height': 2048,\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Data Preparation\n",
    "- Remove duplicated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_removal = DFDuplicateRemoval(target='Class', keep='mean')\n",
    "duplicate_removal.fit(data_df)\n",
    "\n",
    "# Observe duplicated data\n",
    "duplicate_df = duplicate_removal.duplicate_df\n",
    "\n",
    "duplicate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(duplicate_df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe if duplicated data are having different target label\n",
    "vp.value_count(\n",
    "    duplicate_df.groupby(duplicate_removal.subset).agg(\n",
    "        Class=('Class', 'mean')\n",
    "    ).reset_index(),\n",
    "    'Class'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated data\n",
    "data_df = duplicate_removal.transform(data_df)\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(data_df, 'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Classification\n",
    "- Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {\n",
    "    'cv_score': [],\n",
    "    'matrix':   [],\n",
    "    'method':   [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X, y = mlpipe.xy_split(data_df, 'Class')\n",
    "\n",
    "vp.value_count(y.to_frame(), 'Class')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset\n",
    "X_train, X_test, y_train, y_test = mlpipe.dataset_split(X, y, test_size=.3, stratify=y, random_state=0)\n",
    "\n",
    "print('Train dataset:\\n-----------------------')\n",
    "vp.value_count(y_train.to_frame(), 'Class')\n",
    "print('\\nTest dataset:\\n----------------------')\n",
    "vp.value_count(y_test.to_frame(), 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(estimator, X, y):\n",
    "    return cross_val_score(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        scoring='roc_auc',\n",
    "        cv=StratifiedKFold(10),\n",
    "        verbose=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "def train_test_evaluation(method, metric_dict, cv_scores,\n",
    "                          model, X_train, y_train, X_test, y_test,\n",
    "                          train_pipeline, test_pipeline=None):\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(\n",
    "        train_pipeline.fit_transform(X_train),\n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    # Evaluate test data\n",
    "    test_pipeline = train_pipeline if test_pipeline is None else test_pipeline\n",
    "    \n",
    "    eval_dict = mlpipe.eval_classif(\n",
    "        y_test,\n",
    "        model.predict(\n",
    "            test_pipeline.transform(X_test)\n",
    "        ),\n",
    "        y_prob=model.predict_proba(\n",
    "            test_pipeline.transform(X_test)\n",
    "        )[:,-1],\n",
    "        return_evaluation=True\n",
    "    )\n",
    "\n",
    "    metric_dict['method'].append(method)\n",
    "    metric_dict['cv_score'].append(cv_scores)\n",
    "    metric_dict['matrix'].append(eval_dict['matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = DFStandardScaler(columns=['Time', 'Amount'])\n",
    "minmax_scaler   = DFMinMaxScaler()\n",
    "model           = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "\n",
    "# Cross validation\n",
    "scores = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "print(f'CV Score: {np.mean(scores) :.5f} ({np.std(scores) :.5f})\\n')\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Baseline',\n",
    "    metric_dict,\n",
    "    scores,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Classification\n",
    "- Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [\n",
    "    ('Class Weight', mlpipe.class_weight(y_train)),\n",
    "    ('Class Weight (Normalize)', mlpipe.class_weight(y_train, normalize=True)),\n",
    "    ('Class Ratio', mlpipe.class_ratio(y_train)),\n",
    "    ('Class Ratio (Normalize)', mlpipe.class_ratio(y_train, normalize=True)),\n",
    "    ('Class Ratio Floor', mlpipe.class_ratio(y_train, rounding='floor')),\n",
    "    ('Class Ratio Floor (Normalize)', mlpipe.class_ratio(y_train, rounding='floor', normalize=True)),\n",
    "    ('Class Ratio Ceil', mlpipe.class_ratio(y_train, rounding='ceil')),\n",
    "    ('Class Ratio Ceil (Normalize)', mlpipe.class_ratio(y_train, rounding='ceil', normalize=True)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for method, class_weight_dict in class_weights:\n",
    "    print(f'\\n{method}:')\n",
    "    \n",
    "    model = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                               class_weight=class_weight_dict)\n",
    "    \n",
    "    # Cross validation\n",
    "    scores = cross_validation(\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "            ('model', model),\n",
    "        ], verbose=True),\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "    print(f'CV Score: {np.mean(scores) :.5f} ({np.std(scores) :.5f})\\n')\n",
    "\n",
    "    # Evaluation\n",
    "    train_test_evaluation(\n",
    "        method,\n",
    "        metric_dict,\n",
    "        scores,\n",
    "        model,\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        train_pipeline=Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "        ], verbose=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5 - Classification\n",
    "- Re-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    (\n",
    "        'ADASYN + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('adasyn', ADASYN(random_state=0, n_jobs=-1)),\n",
    "            ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'ADASYN + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('adasyn', ADASYN(random_state=0, n_jobs=-1)),\n",
    "            ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SMOTE + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('smote_enn', SMOTEENN(random_state=0, n_jobs=-1)),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SMOTE + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('smote_tomek', SMOTETomek(random_state=0, n_jobs=-1)),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'Borderline SMOTE + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "            ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'Borderline SMOTE + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "            ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SVM SMOTE + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=1000, random_state=0),\n",
    "                                   random_state=0, n_jobs=-1)),\n",
    "            ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SVM SMOTE + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=1000, random_state=0),\n",
    "                                   random_state=0, n_jobs=-1)),\n",
    "            ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for method, resample_pipeline in pipelines:\n",
    "    print(f'\\n{method}:')\n",
    "    \n",
    "    # Cross validation\n",
    "    model = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "    steps = resample_pipeline.steps.copy()\n",
    "    steps.append(('minmax_scaler', minmax_scaler))\n",
    "    steps.append(('model', model))\n",
    "\n",
    "    # Reference: https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html\n",
    "    scores = cross_validation(\n",
    "        Pipeline(steps, verbose=True),\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "    print(f'CV Score: {np.mean(scores) :.5f} ({np.std(scores) :.5f})\\n')\n",
    "    \n",
    "    # Re-sampling\n",
    "    X_bal, y_bal = resample_pipeline.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    train_test_evaluation(\n",
    "        method,\n",
    "        metric_dict,\n",
    "        scores,\n",
    "        model,\n",
    "        X_bal, y_bal, X_test, y_test,\n",
    "        train_pipeline=Pipeline(steps=[\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "        ], verbose=True),\n",
    "        test_pipeline=Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "        ], verbose=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metric_dict, title):\n",
    "    eval_df  = pd.DataFrame(metric_dict)\n",
    "    eval_dfs = []\n",
    "    for index in eval_df.index:\n",
    "        eval_dfs.append(\n",
    "            pd.DataFrame({\n",
    "                'method':   eval_df.at[index, 'method'],\n",
    "                'cv_score': eval_df.at[index, 'cv_score'],\n",
    "            })\n",
    "        )\n",
    "\n",
    "    vp.box(\n",
    "        pd.concat(eval_dfs, axis=0),\n",
    "        color='method',\n",
    "        max_col=1,\n",
    "        title=title,\n",
    "        out_path=OUT_PATH_GRAPH,\n",
    "        layout_kwargs={'showlegend': False},\n",
    "        box_kwargs={\n",
    "            'boxmean': 'sd',\n",
    "            'boxpoints': False,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metric_dict, 'Phase 5 - Box - CV Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(metric_dict, title, z_max=None):\n",
    "    data_groups = []\n",
    "    annotations = ()\n",
    "\n",
    "    for index, matrix_df in enumerate(metric_dict['matrix']):\n",
    "        # Reverse sorting to ensure plotly display is same as dataframe layout\n",
    "        tmp_df = matrix_df.sort_index(ascending=False)\n",
    "        \n",
    "        # Heatmap data\n",
    "        fig = fig = ff.create_annotated_heatmap(\n",
    "            z=tmp_df.values,\n",
    "            x=[f'Pred {x}' for x in tmp_df.columns],\n",
    "            y=[f'True {x}' for x in tmp_df.index],\n",
    "            colorscale='Portland',\n",
    "            zmin=0,\n",
    "            zmax=z_max\n",
    "        )\n",
    "        data_groups.append(fig['data'])\n",
    "\n",
    "        # Heatmap annotation\n",
    "        annotation = fig['layout']['annotations']\n",
    "        for x in annotation:\n",
    "            suffix = '' if index == 0 else index+1\n",
    "            x['xref'] = f'x{suffix}'\n",
    "            x['yref'] = f'y{suffix}'\n",
    "        annotations += annotation\n",
    "\n",
    "    vp.datagroups_subplots(data_groups,\n",
    "                           xaxis_titles=metric_dict['method'],\n",
    "                           max_col=4,\n",
    "                           title=title,\n",
    "                           out_path=OUT_PATH_GRAPH,\n",
    "                           layout_kwargs={\n",
    "                               'height': 1500,\n",
    "                               'annotations': annotations,\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(metric_dict, 'Phase 5 - Confusion Matrix', z_max=y_test.value_counts().values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6 - Classification\n",
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_search(X, y, estimator, param_distributions, n_splits=10):\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator,\n",
    "        param_distributions,\n",
    "        scoring='roc_auc',\n",
    "        cv=StratifiedKFold(n_splits=n_splits),\n",
    "        n_jobs=-1,\n",
    "        verbose=10,\n",
    "        n_iter=100,\n",
    "        random_state=0,\n",
    "        refit=False\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight (Normalize)\n",
    "- Best performance among class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/logistic-regression-model-tuning-with-scikit-learn-part-1-425142e01af5\n",
    "model  = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                            class_weight=mlpipe.class_weight(y_train, normalize=True))\n",
    "search = cv_search(\n",
    "    X_train, y_train,\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    param_distributions={\n",
    "        'model__C': np.logspace(-4, 4, 20),\n",
    "        'model__penalty': ['l2'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(search.cv_results_)\n",
    "result_df[['params', 'mean_test_score', 'rank_test_score']].sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                           class_weight=mlpipe.class_weight(y_train, normalize=True),\n",
    "                           **{k.replace('model__', ''): v for k,v in search.best_params_.items()})\n",
    "\n",
    "# Cross validation\n",
    "scores = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "print(f'CV Score: {np.mean(scores) :.5f} ({np.std(scores) :.5f})\\n')\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'CV Search (Class Weight)',\n",
    "    metric_dict,\n",
    "    scores,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borderline SMOTE + Tomek\n",
    "- 3rd best performance among re-sampling\n",
    "- Top best performance falls on SVM SMOTE + ENN, and SVM SMOTE + Tomek, but it's too time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "search = cv_search(\n",
    "    X_train, y_train,\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "        ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    param_distributions={\n",
    "        'model__C': np.logspace(-4, 4, 20),\n",
    "        'model__penalty': ['l2'],\n",
    "    },\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(search.cv_results_)\n",
    "result_df[['params', 'mean_test_score', 'rank_test_score']].sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                           **{k.replace('model__', ''): v for k,v in search.best_params_.items()})\n",
    "\n",
    "# Cross validation\n",
    "# Reference: https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html\n",
    "scores = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "        ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "print(f'CV Score: {np.mean(scores) :.5f} ({np.std(scores) :.5f})\\n')\n",
    "\n",
    "# Re-sampling\n",
    "X_bal, y_bal = Pipeline(steps=[\n",
    "    ('standard_scaler', standard_scaler),\n",
    "    ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "    ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "], verbose=True).fit_resample(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'CV Search (Borderline SMOTE + Tomek)',\n",
    "    metric_dict,\n",
    "    scores,\n",
    "    model,\n",
    "    X_bal, y_bal, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True),\n",
    "    test_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metric_dict, 'Phase 6 - Box - CV Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(metric_dict, 'Phase 6 - Confusion Matrix', z_max=y_test.value_counts().values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7 - Classification\n",
    "- Model Stacking (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {\n",
    "    'macro_precision': [],\n",
    "    'macro_recall':    [],\n",
    "    'macro_f1':        [],\n",
    "    'roc_auc':         [],\n",
    "    'pr_auc':          [],\n",
    "    'cv_score':        [],\n",
    "    'method':          [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_base_models(estimators, X, y):\n",
    "    eval_dict = {\n",
    "        'model':   [],\n",
    "        'score-m': [],\n",
    "        'score-s': []\n",
    "    }\n",
    "\n",
    "    # Cross validation\n",
    "    for k,v in tqdm(estimators):\n",
    "        scores = cross_validation(\n",
    "            Pipeline(steps=[\n",
    "                ('standard_scaler', standard_scaler),\n",
    "                ('minmax_scaler', minmax_scaler),\n",
    "                ('model', v),\n",
    "            ], verbose=True),\n",
    "            X,\n",
    "            y\n",
    "        )\n",
    "        eval_dict['model'].append(k)\n",
    "        eval_dict['score-m'].append(np.mean(scores))\n",
    "        eval_dict['score-s'].append(np.std(scores))\n",
    "    \n",
    "    eval_df = pd.DataFrame(eval_dict).set_index('model')\n",
    "    eval_df['score-m'] = eval_df['score-m'].round(3)\n",
    "    eval_df['score-s'] = eval_df['score-s'].round(3)\n",
    "    eval_df.index.name = ''\n",
    "    \n",
    "    return eval_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = mlpipe.class_weight(y_train, normalize=True)\n",
    "\n",
    "models = [\n",
    "    OneVsRestClassifier(LinearSVC(class_weight=weight_dict, random_state=0), n_jobs=-1),\n",
    "    OneVsRestClassifier(SVC(max_iter=1000, probability=True, class_weight=weight_dict, random_state=0), n_jobs=-1),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    DummyClassifier(random_state=0),\n",
    "    AdaBoostClassifier(random_state=0),\n",
    "    ExtraTreesClassifier(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    GradientBoostingClassifier(random_state=0),\n",
    "    RandomForestClassifier(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    DecisionTreeClassifier(class_weight=weight_dict, random_state=0),\n",
    "    LogisticRegression(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    PassiveAggressiveClassifier(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    Perceptron(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    RidgeClassifier(class_weight=weight_dict, random_state=0),\n",
    "    SGDClassifier(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    MLPClassifier(random_state=0)\n",
    "]\n",
    "\n",
    "# Final model\n",
    "model = StackingClassifier(\n",
    "    estimators=[\n",
    "        (x.estimator.__class__.__name__ if x.__class__.__name__ == 'OneVsRestClassifier', x)\n",
    "        else (x.__class__.__name__, x)\n",
    "        for x in models\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(class_weight=weight_dict, random_state=0, n_jobs=-1),\n",
    "    cv=StratifiedKFold(10),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross validation (Base Models)\n",
    "eval_df = cv_base_models(model.estimators, X_train, y_train)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation (Meta Model)\n",
    "scores = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "print(f'CV Score: {np.mean(scores) :.5f} ({np.std(scores) :.5f})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Model Stacking (Baseline)',\n",
    "    metric_dict,\n",
    "    scores,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8 - Classification\n",
    "- Model Stacking (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 9 - Classification\n",
    "- Model Stacking (Combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Reference: https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840#:~:text=What%20is%20Stacking%3F,any%20classifier%20of%20your%20choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
