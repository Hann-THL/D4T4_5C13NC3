{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib._util.visualplot as vp\n",
    "import lib._util.mlpipe as mlpipe\n",
    "\n",
    "# Pre-processing\n",
    "from lib._class.DFDuplicateRemoval import DFDuplicateRemoval\n",
    "\n",
    "# Feature scaling\n",
    "from lib._class.DFRobustScaler import DFRobustScaler\n",
    "from lib._class.DFStandardScaler import DFStandardScaler\n",
    "from lib._class.DFMinMaxScaler import DFMinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Time measurement\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Imbalanced-Learn\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import ADASYN, BorderlineSMOTE, SVMSMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours, TomekLinks\n",
    "\n",
    "# Plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_PATH_DATA = 'resources/data/'\n",
    "OUT_PATH_GRAPH   = 'resources/output/graph/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Loading\n",
    "- Reference: https://www.kaggle.com/mlg-ulb/creditcardfraud/home\n",
    "- Time: Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
    "- V1-V28: May be result of a PCA dimensionality reduction to protect user identities and sensitive features\n",
    "- Amount: Transaction amount\n",
    "- Class: 1 for fraudulent transactions, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = pd.read_csv(f'{SOURCE_PATH_DATA}creditcard.csv', sep=',', chunksize=50_000)\n",
    "data_df   = pd.concat(df_chunks)\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(data_df, 'Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.histogram(data_df,\n",
    "             bin_algo='count',\n",
    "             max_col=4,\n",
    "             title='Phase 1 - Histogram',\n",
    "             out_path=OUT_PATH_GRAPH,\n",
    "             layout_kwargs={'height': 2048})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.box(data_df,\n",
    "       color='Class',\n",
    "       max_col=4,\n",
    "       title='Phase 1 - Box',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'height': 2048,\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.kde(data_df,\n",
    "       color='Class',\n",
    "       max_col=4,\n",
    "       title='Phase 1 - KDE',\n",
    "       out_path=OUT_PATH_GRAPH,\n",
    "       layout_kwargs={\n",
    "           'height': 2048,\n",
    "           'legend_orientation': 'h'\n",
    "       })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Data Preparation\n",
    "- Remove duplicated data\n",
    "- Reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_removal = DFDuplicateRemoval(target='Class', keep='mean')\n",
    "duplicate_removal.fit(data_df)\n",
    "\n",
    "# Observe duplicated data\n",
    "duplicate_df = duplicate_removal.duplicate_df\n",
    "\n",
    "duplicate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(duplicate_df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe if duplicated data are having different target label\n",
    "vp.value_count(\n",
    "    duplicate_df.groupby(duplicate_removal.subset).agg(\n",
    "        Class=('Class', 'mean')\n",
    "    ).reset_index(),\n",
    "    'Class'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated data\n",
    "data_df = duplicate_removal.transform(data_df)\n",
    "\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp.value_count(data_df, 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - change to class\n",
    "data_df = mlpipe.reduce_memory_usage(data_df)\n",
    "\n",
    "vp.faststat(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Classification\n",
    "- Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {\n",
    "    'roc_auc':  [],\n",
    "    'f1_macro': [],\n",
    "    'cv_score': [],\n",
    "    'matrix':   [],\n",
    "    'method':   [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features & target\n",
    "X, y = mlpipe.xy_split(data_df, 'Class')\n",
    "\n",
    "vp.value_count(y.to_frame(), 'Class')\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate dataset\n",
    "X_train, X_test, y_train, y_test = mlpipe.dataset_split(X, y, test_size=.8, stratify=y, random_state=0)\n",
    "\n",
    "print('Train dataset:\\n-----------------------')\n",
    "vp.value_count(y_train.to_frame(), 'Class')\n",
    "print('\\nTest dataset:\\n----------------------')\n",
    "vp.value_count(y_test.to_frame(), 'Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_sampling(metric_dict, indexes):\n",
    "    new_dict = {}\n",
    "    for key in metric_dict.keys():\n",
    "        new_dict[key] = [x for i,x in enumerate(metric_dict[key]) if i in indexes]\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "def cross_validation(estimator, X, y, n_splits=10, show_score=True):\n",
    "    cv_dict = cross_validate(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        scoring=['f1_macro', 'roc_auc'],\n",
    "        cv=StratifiedKFold(n_splits),\n",
    "        verbose=10,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    if show_score:\n",
    "        cv_roc_auc  = cv_dict['test_roc_auc']\n",
    "        cv_f1_macro = cv_dict['test_f1_macro']\n",
    "        print(f'CV ROC-AUC:  {np.mean(cv_roc_auc) :.5f} ({np.std(cv_roc_auc) :.5f})')\n",
    "        print(f'CV F1-Macro: {np.mean(cv_f1_macro) :.5f} ({np.std(cv_f1_macro) :.5f})\\n')\n",
    "    \n",
    "    return cv_dict\n",
    "\n",
    "def train_test_evaluation(method, metric_dict, cv_dict,\n",
    "                          model, X_train, y_train, X_test, y_test,\n",
    "                          train_pipeline=None, test_pipeline=None):\n",
    "    \n",
    "    # Model training\n",
    "    model.fit(\n",
    "        X_train if train_pipeline is None else train_pipeline.fit_transform(X_train),\n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    # Evaluate test data\n",
    "    test_pipeline = train_pipeline if test_pipeline is None else test_pipeline\n",
    "    \n",
    "    eval_dict = mlpipe.eval_classif(\n",
    "        y_test,\n",
    "        model.predict(\n",
    "            X_test if test_pipeline is None else test_pipeline.transform(X_test)\n",
    "        ),\n",
    "        y_prob=model.predict_proba(\n",
    "            X_test if test_pipeline is None else test_pipeline.transform(X_test)\n",
    "        )[:,-1],\n",
    "        return_evaluation=True\n",
    "    )\n",
    "\n",
    "    metric_dict['method'].append(method)\n",
    "    metric_dict['roc_auc'].append(cv_dict['test_roc_auc'])\n",
    "    metric_dict['f1_macro'].append(cv_dict['test_f1_macro'])\n",
    "    metric_dict['cv_score'].append((cv_dict['test_roc_auc'] + cv_dict['test_f1_macro']) / 2)\n",
    "    metric_dict['matrix'].append(eval_dict['matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = DFStandardScaler(columns=['Time', 'Amount'])\n",
    "minmax_scaler   = DFMinMaxScaler()\n",
    "model           = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "\n",
    "# Cross validation\n",
    "cv_dict = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Baseline',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4 - Classification\n",
    "- Class Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [\n",
    "    ('Weight', mlpipe.class_weight(y_train)),\n",
    "    ('Weight (Normalize)', mlpipe.class_weight(y_train, normalize=True)),\n",
    "    ('Ratio', mlpipe.class_ratio(y_train)),\n",
    "    ('Ratio (Normalize)', mlpipe.class_ratio(y_train, normalize=True)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for method, weight_dict in class_weights:\n",
    "    print(f'\\n{method}:')\n",
    "    \n",
    "    model = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                               class_weight=weight_dict)\n",
    "    \n",
    "    # Cross validation\n",
    "    cv_dict = cross_validation(\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "            ('model', model),\n",
    "        ], verbose=True),\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    train_test_evaluation(\n",
    "        method,\n",
    "        metric_dict,\n",
    "        cv_dict,\n",
    "        model,\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        train_pipeline=Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "        ], verbose=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score(metric_dict, title, layout_kwargs={}):\n",
    "    eval_df  = pd.DataFrame(metric_dict)\n",
    "    eval_dfs = []\n",
    "    for index in eval_df.index:\n",
    "        eval_dfs.append(\n",
    "            pd.DataFrame({\n",
    "                'method':   eval_df.at[index, 'method'],\n",
    "                'cv_score': eval_df.at[index, 'cv_score'],\n",
    "                'f1_macro': eval_df.at[index, 'f1_macro'],\n",
    "                'roc_auc':  eval_df.at[index, 'roc_auc'],\n",
    "            })\n",
    "        )\n",
    "\n",
    "    vp.box(\n",
    "        pd.concat(eval_dfs, axis=0),\n",
    "        color='method',\n",
    "        max_col=1,\n",
    "        title=title,\n",
    "        out_path=OUT_PATH_GRAPH,\n",
    "        layout_kwargs={**layout_kwargs, 'showlegend': False},\n",
    "        box_kwargs={\n",
    "            'boxmean': 'sd',\n",
    "            'boxpoints': False,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(metric_dict,\n",
    "           'Phase 4 - Box - Class Weighting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matrix(metric_dict, title, z_max=None, max_col=4, layout_kwargs={}):\n",
    "    data_groups = []\n",
    "    annotations = ()\n",
    "\n",
    "    for index, matrix_df in enumerate(metric_dict['matrix']):\n",
    "        # Reverse sorting to ensure plotly display is same as dataframe layout\n",
    "        tmp_df = matrix_df.sort_index(ascending=False)\n",
    "        \n",
    "        # Heatmap data\n",
    "        fig = fig = ff.create_annotated_heatmap(\n",
    "            z=tmp_df.values,\n",
    "            x=[f'Pred {x}' for x in tmp_df.columns],\n",
    "            y=[f'True {x}' for x in tmp_df.index],\n",
    "            colorscale='Portland',\n",
    "            zmin=0,\n",
    "            zmax=z_max\n",
    "        )\n",
    "        data_groups.append(fig['data'])\n",
    "\n",
    "        # Heatmap annotation\n",
    "        annotation = fig['layout']['annotations']\n",
    "        for x in annotation:\n",
    "            suffix = '' if index == 0 else index+1\n",
    "            x['xref'] = f'x{suffix}'\n",
    "            x['yref'] = f'y{suffix}'\n",
    "        annotations += annotation\n",
    "\n",
    "    vp.datagroups_subplots(data_groups,\n",
    "                           xaxis_titles=metric_dict['method'],\n",
    "                           max_col=max_col,\n",
    "                           title=title,\n",
    "                           out_path=OUT_PATH_GRAPH,\n",
    "                           layout_kwargs={\n",
    "                               'annotations': annotations,\n",
    "                               **layout_kwargs,\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(metric_dict,\n",
    "            'Phase 4 - Confusion Matrix - Class Weighting',\n",
    "            z_max=y_test.value_counts().values[-1],\n",
    "            max_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5 - Classification\n",
    "- Re-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    (\n",
    "        'ADASYN + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('adasyn', ADASYN(random_state=0, n_jobs=-1)),\n",
    "            ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'ADASYN + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('adasyn', ADASYN(random_state=0, n_jobs=-1)),\n",
    "            ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SMOTE + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('smote_enn', SMOTEENN(random_state=0, n_jobs=-1)),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SMOTE + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('smote_tomek', SMOTETomek(random_state=0, n_jobs=-1)),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'BL SMOTE + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "            ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'BL SMOTE + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('borderline_smote', BorderlineSMOTE(random_state=0, n_jobs=-1)),\n",
    "            ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SVM SMOTE + ENN',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=200, random_state=0),\n",
    "                                   random_state=0, n_jobs=-1)),\n",
    "            ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "    (\n",
    "        'SVM SMOTE + Tomek',\n",
    "        Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=200, random_state=0),\n",
    "                                   random_state=0, n_jobs=-1)),\n",
    "            ('tomek', TomekLinks(n_jobs=-1, sampling_strategy='all')),\n",
    "        ], verbose=True)\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for method, resample_pipeline in pipelines:\n",
    "    print(f'\\n{method}:')\n",
    "    \n",
    "    # Cross validation\n",
    "    model = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "    steps = resample_pipeline.steps.copy()\n",
    "    steps.append(('minmax_scaler', minmax_scaler))\n",
    "    steps.append(('model', model))\n",
    "\n",
    "    # Reference: https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html\n",
    "    cv_dict = cross_validation(\n",
    "        Pipeline(steps, verbose=True),\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "    \n",
    "    # Re-sampling\n",
    "    X_bal, y_bal = resample_pipeline.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Evaluation\n",
    "    train_test_evaluation(\n",
    "        method,\n",
    "        metric_dict,\n",
    "        cv_dict,\n",
    "        model,\n",
    "        X_bal, y_bal, X_test, y_test,\n",
    "        train_pipeline=Pipeline(steps=[\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "        ], verbose=True),\n",
    "        test_pipeline=Pipeline(steps=[\n",
    "            ('standard_scaler', standard_scaler),\n",
    "            ('minmax_scaler', minmax_scaler),\n",
    "        ], verbose=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(dict_sampling(metric_dict, [0] + list(np.linspace(5, 12, 8))),\n",
    "           'Phase 5 - Box - Re-sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(dict_sampling(metric_dict, [0] + list(np.linspace(5, 12, 8))),\n",
    "            'Phase 5 - Confusion Matrix - Re-sampling',\n",
    "            z_max=y_test.value_counts().values[-1],\n",
    "            layout_kwargs={'height': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6 - Classification\n",
    "- Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_search(X, y, estimator, param_distributions, n_splits=10, n_iter=100, cv=None):\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator,\n",
    "        param_distributions,\n",
    "        scoring=['f1_macro', 'roc_auc'],\n",
    "        cv=StratifiedKFold(n_splits=n_splits) if cv is None else cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=10,\n",
    "        n_iter=n_iter,\n",
    "        random_state=0,\n",
    "        refit=False\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    \n",
    "    return search\n",
    "\n",
    "def cv_results(search):\n",
    "    result_df = pd.DataFrame(search.cv_results_)\n",
    "    result_df = result_df[[\n",
    "        'params',\n",
    "        'mean_test_f1_macro', 'rank_test_f1_macro',\n",
    "        'mean_test_roc_auc', 'rank_test_roc_auc'\n",
    "    ]]\n",
    "    result_df['cv_score'] = result_df[['mean_test_f1_macro', 'mean_test_roc_auc']].sum(axis=1) / 2\n",
    "    result_df.sort_values(by='cv_score', ascending=False, inplace=True)\n",
    "    best_params = result_df['params'].values[0]\n",
    "    \n",
    "    return result_df, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_params = {\n",
    "    # 'model__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    'model__fit_intercept': [True, False],\n",
    "    'model__max_iter': [1000],\n",
    "    'model__multi_class': ['auto'],\n",
    "}\n",
    "\n",
    "search_params = [\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'model__penalty': ['l1'],\n",
    "    },\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "        'model__penalty': ['l2'],\n",
    "    },\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__solver': ['liblinear'],\n",
    "        'model__penalty': ['l2'],\n",
    "        'model__dual': [True, False],\n",
    "    },\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__solver': ['saga'],\n",
    "        'model__penalty': ['elasticnet'],\n",
    "        'model__l1_ratio': np.linspace(.1, .9, 9),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight (Normalize)\n",
    "- Best performance among class weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find best solver combination (low cardinality)\n",
    "model  = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                            class_weight=mlpipe.class_weight(y_train, normalize=True))\n",
    "search = cv_search(\n",
    "    X_train, y_train,\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    search_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, best_params = cv_results(search)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best C value corresponding to solver combination (high cardinality)\n",
    "search_params = [\n",
    "    {\n",
    "        **{k: [v] for k,v in best_params.items()},\n",
    "        'model__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "model  = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                            class_weight=mlpipe.class_weight(y_train, normalize=True))\n",
    "search = cv_search(\n",
    "    X_train, y_train,\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    search_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, best_params = cv_results(search)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                           class_weight=mlpipe.class_weight(y_train, normalize=True),\n",
    "                           **{k.replace('model__', ''): v for k,v in best_params.items()})\n",
    "\n",
    "# Cross validation\n",
    "cv_dict = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Weight (Normalize) - Tuned',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM SMOTE + ENN\n",
    "- Best performance among re-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best C value corresponding to solver combination (high cardinality)\n",
    "search_params = [\n",
    "    {\n",
    "        **{k: [v] for k,v in best_params.items() if k != 'model__C'},\n",
    "        'model__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "model  = LogisticRegression(random_state=0, n_jobs=-1)\n",
    "search = cv_search(\n",
    "    X_train, y_train,\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=200, random_state=0),\n",
    "                               random_state=0, n_jobs=-1)),\n",
    "        ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    search_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, best_params = cv_results(search)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0, n_jobs=-1,\n",
    "                           **{k.replace('model__', ''): v for k,v in best_params.items()})\n",
    "\n",
    "# Cross validation\n",
    "# Reference: https://kiwidamien.github.io/how-to-do-cross-validation-when-upsampling-data.html\n",
    "cv_dict = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=200, random_state=0),\n",
    "                               random_state=0, n_jobs=-1)),\n",
    "        ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train\n",
    ")\n",
    "\n",
    "# Re-sampling\n",
    "X_bal, y_bal = Pipeline(steps=[\n",
    "    ('standard_scaler', standard_scaler),\n",
    "    ('svm_smote', SVMSMOTE(svm_estimator=SVC(class_weight='balanced', max_iter=200, random_state=0),\n",
    "                           random_state=0, n_jobs=-1)),\n",
    "    ('enn', EditedNearestNeighbours(n_jobs=-1, sampling_strategy='all')),\n",
    "], verbose=True).fit_resample(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'SVM SMOTE + ENN - Tuned',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_bal, y_bal, X_test, y_test,\n",
    "    train_pipeline=Pipeline(steps=[\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True),\n",
    "    test_pipeline=Pipeline(steps=[\n",
    "        ('standard_scaler', standard_scaler),\n",
    "        ('minmax_scaler', minmax_scaler),\n",
    "    ], verbose=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(dict_sampling(metric_dict, [0, 2, 11, 13, 14]),\n",
    "           'Phase 6 - Box - Tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(dict_sampling(metric_dict, [0, 2, 11, 13, 14]),\n",
    "            'Phase 6 - Confusion Matrix - Tuned',\n",
    "            z_max=y_test.value_counts().values[-1],\n",
    "            max_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7 - Classification\n",
    "- Model Stacking (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(name, class_weight=None, params={}):\n",
    "    kwargs1    = {'random_state': 0, 'n_jobs': -1}\n",
    "    kwargs1_w  = {**kwargs1, 'class_weight': class_weight}\n",
    "    kwargs1_w2 = {**kwargs1, 'scale_pos_weight': None if class_weight is None else class_weight[1]}\n",
    "    kwargs2    = {'n_jobs': -1}\n",
    "    kwargs2_w  = {**kwargs2, 'class_weight': class_weight}\n",
    "    kwargs3    = {'random_state': 0}\n",
    "    kwargs3_w  = {**kwargs3, 'class_weight': class_weight}\n",
    "    kwargs3_w2 = {**kwargs3_w, 'probability': True}\n",
    "    \n",
    "    if name == 'KNeighborsClassifier':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            KNeighborsClassifier(**kwargs2, **params)\n",
    "        )\n",
    "    \n",
    "    if name == 'LinearSVC':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            CalibratedClassifierCV(OneVsRestClassifier(LinearSVC(**kwargs3_w, **params), **kwargs2))\n",
    "        )\n",
    "    \n",
    "    if name == 'NuSVC':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            OneVsRestClassifier(NuSVC(**kwargs3_w2, **params), **kwargs2)\n",
    "        )\n",
    "    \n",
    "    if name == 'SVC':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            OneVsRestClassifier(SVC(**kwargs3_w2, **params), **kwargs2)\n",
    "        )\n",
    "    \n",
    "    if name == 'XGBClassifier':\n",
    "        return XGBClassifier(**kwargs1_w2, **params)\n",
    "    \n",
    "    if name == 'GradientBoostingClassifier':\n",
    "        return GradientBoostingClassifier(**kwargs3, **params)\n",
    "    \n",
    "    if name == 'ExtraTreesClassifier':\n",
    "        return ExtraTreesClassifier(**kwargs1_w, **params)\n",
    "    \n",
    "    if name == 'RandomForestClassifier':\n",
    "        return RandomForestClassifier(**kwargs1_w, **params)\n",
    "    \n",
    "    if name == 'DecisionTreeClassifier':\n",
    "        return DecisionTreeClassifier(**kwargs3_w, **params)\n",
    "    \n",
    "    if name == 'ExtraTreeClassifier':\n",
    "        return ExtraTreeClassifier(**kwargs3_w, **params)\n",
    "    \n",
    "    if name == 'LogisticRegression':\n",
    "        return make_pipeline(\n",
    "            DFRobustScaler(),\n",
    "            DFMinMaxScaler(),\n",
    "            LogisticRegression(**kwargs1_w, **params)\n",
    "        )\n",
    "    \n",
    "    if name == 'PassiveAggressiveClassifier':\n",
    "        return make_pipeline(\n",
    "            DFRobustScaler(),\n",
    "            DFMinMaxScaler(),\n",
    "            CalibratedClassifierCV(PassiveAggressiveClassifier(**kwargs1_w, **params))\n",
    "        )\n",
    "    \n",
    "    if name == 'Perceptron':\n",
    "        return make_pipeline(\n",
    "            DFRobustScaler(),\n",
    "            DFMinMaxScaler(),\n",
    "            CalibratedClassifierCV(Perceptron(**kwargs1_w, **params))\n",
    "        )\n",
    "    \n",
    "    if name == 'RidgeClassifier':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            CalibratedClassifierCV(RidgeClassifier(**kwargs3_w, **params))\n",
    "        )\n",
    "    \n",
    "    if name == 'SGDClassifier':\n",
    "        return make_pipeline(\n",
    "            DFRobustScaler(),\n",
    "            DFMinMaxScaler(),\n",
    "            SGDClassifier(**kwargs1_w, **params)\n",
    "        )\n",
    "    \n",
    "    if name == 'LinearDiscriminantAnalysis':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            LinearDiscriminantAnalysis(**params)\n",
    "        )\n",
    "    \n",
    "    if name == 'QuadraticDiscriminantAnalysis':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            QuadraticDiscriminantAnalysis(**params)\n",
    "        )\n",
    "    \n",
    "    if name == 'MLPClassifier':\n",
    "        return make_pipeline(\n",
    "            DFRobustScaler(),\n",
    "            DFMinMaxScaler(),\n",
    "            MLPClassifier(**kwargs3, **params)\n",
    "        )\n",
    "    \n",
    "    if name == 'GaussianProcessClassifier':\n",
    "        return make_pipeline(\n",
    "            DFStandardScaler(),\n",
    "            GaussianProcessClassifier(**kwargs1, **params)\n",
    "        )\n",
    "    \n",
    "    if name == 'BernoulliNB':\n",
    "        return BernoulliNB(**params)\n",
    "    \n",
    "    if name == 'CategoricalNB':\n",
    "        return CategoricalNB(**params)\n",
    "    \n",
    "    if name == 'ComplementNB':\n",
    "        return ComplementNB(**params)\n",
    "    \n",
    "    if name == 'GaussianNB':\n",
    "        return GaussianNB(**params)\n",
    "    \n",
    "    if name == 'MultinomialNB':\n",
    "        return MultinomialNB(**params)\n",
    "    \n",
    "    if name == 'DummyClassifier':\n",
    "        return DummyClassifier(**kwargs3, **params)\n",
    "    \n",
    "    raise Exception(f'{name} not found.')\n",
    "\n",
    "def model_name(model):\n",
    "    name = model.__class__.__name__\n",
    "    if name == 'RandomForestClassifier':\n",
    "        return name\n",
    "    \n",
    "    if hasattr(model, 'estimator'):\n",
    "        return model_name(model.estimator)\n",
    "\n",
    "    elif hasattr(model, 'base_estimator'):\n",
    "        return model_name(model.base_estimator)\n",
    "\n",
    "    return name\n",
    "\n",
    "def get_estimators(class_weight=None):\n",
    "    estimators = [\n",
    "        get_estimator('LinearSVC', class_weight),\n",
    "        get_estimator('XGBClassifier', class_weight),\n",
    "        get_estimator('GradientBoostingClassifier', class_weight),\n",
    "        get_estimator('RandomForestClassifier', class_weight),\n",
    "        get_estimator('PassiveAggressiveClassifier', class_weight),\n",
    "        get_estimator('QuadraticDiscriminantAnalysis', class_weight)\n",
    "    ]\n",
    "    \n",
    "    return [(model_name(x.steps[-1][1]) if type(x) == SklearnPipeline else model_name(x), x) for x in estimators]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_base_models(estimators, X, y):\n",
    "    eval_dict = {\n",
    "        'model':         [],\n",
    "        'mean_roc_auc':  [],\n",
    "        'std_roc_auc':   [],\n",
    "        'mean_f1_macro': [],\n",
    "        'std_f1_macro':  [],\n",
    "    }\n",
    "\n",
    "    # Cross validation\n",
    "    for name, estimator in estimators:\n",
    "        print(name)\n",
    "        \n",
    "        cv_dict    = cross_validation(estimator, X, y, show_score=False)\n",
    "        roc_scores = cv_dict['test_roc_auc']\n",
    "        f1_scores  = cv_dict['test_f1_macro']\n",
    "        \n",
    "        eval_dict['model'].append(name)\n",
    "        eval_dict['mean_roc_auc'].append(np.mean(roc_scores))\n",
    "        eval_dict['mean_f1_macro'].append(np.mean(f1_scores))\n",
    "        eval_dict['std_roc_auc'].append(np.std(roc_scores))\n",
    "        eval_dict['std_f1_macro'].append(np.std(f1_scores))\n",
    "    \n",
    "    eval_df = pd.DataFrame(eval_dict).set_index('model')\n",
    "    eval_df.index.name = ''\n",
    "    \n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = mlpipe.class_weight(y_train, normalize=True)\n",
    "\n",
    "# Final model\n",
    "model = StackingClassifier(\n",
    "    estimators=get_estimators(\n",
    "        weight_dict\n",
    "    ),\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross validation (Base Models)\n",
    "eval_df = cv_base_models(model.estimators, X_train, y_train)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation (Meta Model)\n",
    "cv_dict = cross_validation(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_taken(seconds):\n",
    "    print(f'\\nTime Taken: {str(timedelta(seconds=seconds))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Model Stacking (Baseline)',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8 - Classification\n",
    "- Model Stacking (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best combination (low cardinality)\n",
    "linearsvc_params = {\n",
    "    # 'calibratedclassifiercv__base_estimator__estimator__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    'calibratedclassifiercv__base_estimator__estimator__fit_intercept': [True, False],\n",
    "    'calibratedclassifiercv__base_estimator__estimator__max_iter': [1000],\n",
    "}\n",
    "tree_params = {\n",
    "    # 'max_depth': [None, 3, 5, 7, 10, 15],\n",
    "    # 'min_samples_split': np.linspace(.1, 1, 10),\n",
    "    # 'min_samples_leaf': np.linspace(.1, .5, 5),\n",
    "    # 'max_features': ['sqrt', 'log2'] + list(np.linspace(.5, 1, 6)),\n",
    "}\n",
    "\n",
    "\n",
    "search_params = [\n",
    "    # LinearSVC\n",
    "    [\n",
    "        {\n",
    "            **linearsvc_params,\n",
    "            'calibratedclassifiercv__base_estimator__estimator__multi_class': ['ovr'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__penalty': ['l2'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__loss': ['squared_hinge'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__dual': [True, False],\n",
    "        },\n",
    "        {\n",
    "            **linearsvc_params,\n",
    "            'calibratedclassifiercv__base_estimator__estimator__multi_class': ['ovr'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__penalty': ['l2'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__loss': ['hinge'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__dual': [True],\n",
    "        },\n",
    "        {\n",
    "            **linearsvc_params,\n",
    "            'calibratedclassifiercv__base_estimator__estimator__multi_class': ['ovr'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__penalty': ['l1'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__loss': ['squared_hinge'],\n",
    "            'calibratedclassifiercv__base_estimator__estimator__dual': [False],\n",
    "        },\n",
    "        {\n",
    "            **linearsvc_params,\n",
    "            'calibratedclassifiercv__base_estimator__estimator__multi_class': ['crammer_singer'],\n",
    "        }\n",
    "    ],\n",
    "    # XGBClassifier\n",
    "    [\n",
    "        {\n",
    "            'n_estimators': [250],\n",
    "            # 'max_depth': [None, 3, 5, 7, 10, 15],\n",
    "            'learning_rate': [.1, .01],\n",
    "            'objective':['reg:logistic'],\n",
    "            'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "            'subsample': [.8],\n",
    "            # 'colsample_bytree': np.linspace(.5, 1, 6),\n",
    "            # 'reg_alpha': np.linspace(0, 1, 11),\n",
    "            # 'reg_lambda': np.linspace(0, 1, 11),\n",
    "        }\n",
    "    ],\n",
    "    # GradientBoostingClassifier\n",
    "    [\n",
    "        {\n",
    "            **tree_params,\n",
    "            'criterion': ['friedman_mse'],\n",
    "            'n_estimators': [250],\n",
    "            'loss': ['deviance', 'exponential'],\n",
    "            'learning_rate': [.1, .01],\n",
    "            # 'subsample': np.linspace(.5, 1, 6),\n",
    "        }\n",
    "    ],\n",
    "    # RandomForestClassifier\n",
    "    [\n",
    "        {\n",
    "            **tree_params,\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'n_estimators': [250],\n",
    "            'bootstrap': [False],\n",
    "        },\n",
    "        {\n",
    "            **tree_params,\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'n_estimators': [250],\n",
    "            'bootstrap': [True],\n",
    "            # 'max_samples': np.linspace(.5, 1, 6),\n",
    "        }\n",
    "    ],\n",
    "    # PassiveAggressiveClassifier\n",
    "    [\n",
    "        {\n",
    "            # 'calibratedclassifiercv__base_estimator__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "            'calibratedclassifiercv__base_estimator__fit_intercept': [True, False],\n",
    "            'calibratedclassifiercv__base_estimator__max_iter': [1000],\n",
    "            'calibratedclassifiercv__base_estimator__loss': ['hinge', 'squared_hinge'],\n",
    "            'calibratedclassifiercv__base_estimator__average': [True, False],\n",
    "        }\n",
    "    ],\n",
    "    # QuadraticDiscriminantAnalysis\n",
    "    [\n",
    "        {\n",
    "            'quadraticdiscriminantanalysis__reg_param': np.linspace(0, 1, 11),\n",
    "        }\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    'model':      [],\n",
    "    'roc_auc':    [],\n",
    "    'f1_macro':   [],\n",
    "    'best_param': [],\n",
    "}\n",
    "weight_dict = mlpipe.class_weight(y_train, normalize=True)\n",
    "\n",
    "for index, (name, estimator) in enumerate(get_estimators(weight_dict)):\n",
    "    print(name)\n",
    "    eval_dict['model'].append(name)\n",
    "    \n",
    "    # Cross validation search\n",
    "    search = cv_search(\n",
    "        X_train, y_train,\n",
    "        estimator,\n",
    "        search_params[index]\n",
    "    )\n",
    "    result_df, best_params = cv_results(search)\n",
    "    \n",
    "    try:\n",
    "        cutoff_index = list(best_params.keys())[0].rindex('__')\n",
    "    except ValueError:\n",
    "        cutoff_index = None\n",
    "    \n",
    "    best_estimator = get_estimator(\n",
    "        name,\n",
    "        weight_dict,\n",
    "        best_params if cutoff_index is None else {k[cutoff_index +2:]: v for k,v in best_params.items()}\n",
    "    )\n",
    "    \n",
    "    cv_dict = cross_validation(best_estimator, X_train, y_train, show_score=False)\n",
    "    eval_dict['best_param'].append(best_params)\n",
    "    eval_dict['roc_auc'].append(cv_dict['test_roc_auc'])\n",
    "    eval_dict['f1_macro'].append(cv_dict['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcard_df = pd.DataFrame(eval_dict)\n",
    "lowcard_df['mean_roc_auc']  = lowcard_df['roc_auc'].apply(np.mean)\n",
    "lowcard_df['std_roc_auc']   = lowcard_df['roc_auc'].apply(np.std)\n",
    "lowcard_df['mean_f1_macro'] = lowcard_df['f1_macro'].apply(np.mean)\n",
    "lowcard_df['std_f1_macro']  = lowcard_df['f1_macro'].apply(np.std)\n",
    "lowcard_df.drop(columns=['roc_auc', 'f1_macro'], inplace=True)\n",
    "\n",
    "lowcard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best combination (high cardinality)\n",
    "tree_params = {\n",
    "    'max_depth': [None, 3, 5, 7, 10, 15],\n",
    "    'min_samples_split': np.linspace(.1, 1, 10),\n",
    "    'min_samples_leaf': np.linspace(.1, .5, 5),\n",
    "    'max_features': ['sqrt', 'log2'] + list(np.linspace(.5, 1, 6)),\n",
    "}\n",
    "\n",
    "\n",
    "search_params = [\n",
    "    # LinearSVC\n",
    "    {\n",
    "        'calibratedclassifiercv__base_estimator__estimator__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    },\n",
    "    # XGBClassifier\n",
    "    {\n",
    "        'max_depth': [None, 3, 5, 7, 10, 15],\n",
    "        'colsample_bytree': np.linspace(.5, 1, 6),\n",
    "        'reg_alpha': np.linspace(0, 1, 11),\n",
    "        'reg_lambda': np.linspace(0, 1, 11),\n",
    "    },\n",
    "    # GradientBoostingClassifier\n",
    "    {\n",
    "        **tree_params,\n",
    "        'subsample': np.linspace(.5, 1, 6),\n",
    "    },\n",
    "    # RandomForestClassifier\n",
    "    {\n",
    "        **tree_params,\n",
    "        'max_samples': np.linspace(.5, 1, 6),\n",
    "    },\n",
    "    # PassiveAggressiveClassifier\n",
    "    {\n",
    "        'calibratedclassifiercv__base_estimator__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    },\n",
    "    # QuadraticDiscriminantAnalysis\n",
    "    {\n",
    "        # No high cardinality hyperparameters\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = {\n",
    "    'model':      [],\n",
    "    'roc_auc':    [],\n",
    "    'f1_macro':   [],\n",
    "    'best_param': [],\n",
    "}\n",
    "weight_dict = mlpipe.class_weight(y_train, normalize=True)\n",
    "\n",
    "for index, (name, estimator) in enumerate(get_estimators(weight_dict)):\n",
    "    print(name)\n",
    "    eval_dict['model'].append(name)\n",
    "    \n",
    "    # Cross validation search\n",
    "    search = cv_search(\n",
    "        X_train, y_train,\n",
    "        estimator,\n",
    "        {\n",
    "            **search_params[index],\n",
    "            # Include best low cardinality hyperparameters\n",
    "            **{k: [v] for k,v in lowcard_df['best_param'][index].items()}\n",
    "        }\n",
    "    )\n",
    "    result_df, best_params = cv_results(search)\n",
    "    \n",
    "    try:\n",
    "        cutoff_index = list(best_params.keys())[0].rindex('__')\n",
    "    except ValueError:\n",
    "        cutoff_index = None\n",
    "    \n",
    "    best_params    = best_params if cutoff_index is None else \\\n",
    "                     {k[cutoff_index +2:]: v for k,v in best_params.items()}\n",
    "    best_estimator = get_estimator(\n",
    "        name,\n",
    "        weight_dict,\n",
    "        best_params,\n",
    "    )\n",
    "    \n",
    "    cv_dict = cross_validation(best_estimator, X_train, y_train, show_score=False)\n",
    "    eval_dict['best_param'].append(best_params)\n",
    "    eval_dict['roc_auc'].append(cv_dict['test_roc_auc'])\n",
    "    eval_dict['f1_macro'].append(cv_dict['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highcard_df = pd.DataFrame(eval_dict)\n",
    "highcard_df['mean_roc_auc']  = highcard_df['roc_auc'].apply(np.mean)\n",
    "highcard_df['std_roc_auc']   = highcard_df['roc_auc'].apply(np.std)\n",
    "highcard_df['mean_f1_macro'] = highcard_df['f1_macro'].apply(np.mean)\n",
    "highcard_df['std_f1_macro']  = highcard_df['f1_macro'].apply(np.std)\n",
    "highcard_df.drop(columns=['roc_auc', 'f1_macro'], inplace=True)\n",
    "\n",
    "highcard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base estimators with best hyperparameters\n",
    "estimators  = []\n",
    "weight_dict = mlpipe.class_weight(y_train, normalize=True)\n",
    "\n",
    "for index in tqdm(highcard_df.index):\n",
    "    name   = highcard_df.at[index, 'model']\n",
    "    params = highcard_df.at[index, 'best_param']\n",
    "    estimators.append(\n",
    "        get_estimator(name, weight_dict, params)\n",
    "    )\n",
    "estimators = [(model_name(x.steps[-1][1]) if type(x) == SklearnPipeline else model_name(x), x) for x in estimators]\n",
    "\n",
    "# Final model\n",
    "model = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation (Meta Model)\n",
    "cv_dict = cross_validation(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_splits=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Model Stacking (Base Tuned)',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 9 - Classification\n",
    "- Model Stacking (Combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/stacking-classifiers-for-higher-predictive-performance-566f963e4840#:~:text=What%20is%20Stacking%3F,any%20classifier%20of%20your%20choice.\n",
    "sub_estimators = []\n",
    "min_estimator  = 2\n",
    "max_estimator  = len(estimators)\n",
    "\n",
    "for n_sub in sorted(range(min_estimator, max_estimator +1), reverse=True):\n",
    "    for subset in itertools.combinations(estimators, n_sub):\n",
    "        sub_estimators.append(subset)\n",
    "\n",
    "len(sub_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best base-models combination\n",
    "search_params = [\n",
    "    {\n",
    "        'model__estimators': sub_estimators,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Final model\n",
    "model = StackingClassifier(\n",
    "    estimators=[],\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Cross validation search\n",
    "search = cv_search(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True),\n",
    "    Pipeline(steps=[\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    search_params,\n",
    "    cv=[(X_train.index, X_test.index + len(X_train))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, best_estimators = cv_results(search)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "model = StackingClassifier(\n",
    "    **{k.replace('model__', ''): v for k,v in best_estimators.items()},\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXEC_START = time.time()\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Model Stacking (Base Combination)',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test\n",
    ")\n",
    "\n",
    "EXEC_END = time.time()\n",
    "time_taken(EXEC_END - EXEC_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 10 - Classification\n",
    "- Model Stacking (Meta Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_params = {\n",
    "    # 'model__final_estimator__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    'model__final_estimator__fit_intercept': [True, False],\n",
    "    'model__final_estimator__max_iter': [1000],\n",
    "    'model__final_estimator__multi_class': ['auto'],\n",
    "}\n",
    "\n",
    "search_params = [\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__final_estimator__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'model__final_estimator__penalty': ['l1'],\n",
    "    },\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__final_estimator__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "        'model__final_estimator__penalty': ['l2'],\n",
    "    },\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__final_estimator__solver': ['liblinear'],\n",
    "        'model__final_estimator__penalty': ['l2'],\n",
    "        'model__final_estimator__dual': [True, False],\n",
    "    },\n",
    "    {\n",
    "        **logistic_params,\n",
    "        'model__final_estimator__solver': ['saga'],\n",
    "        'model__final_estimator__penalty': ['elasticnet'],\n",
    "        'model__final_estimator__l1_ratio': np.linspace(.1, .9, 9),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best solver combination (low cardinality)\n",
    "model = StackingClassifier(\n",
    "    **{k.replace('model__', ''): v for k,v in best_estimators.items()},\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Cross validation search\n",
    "search = cv_search(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True),\n",
    "    Pipeline(steps=[\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    search_params,\n",
    "    cv=[(X_train.index, X_test.index + len(X_train))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, best_params = cv_results(search)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best C value corresponding to solver combination (high cardinality)\n",
    "search_params = [\n",
    "    {\n",
    "        **{k: [v] for k,v in best_params.items()},\n",
    "        'model__final_estimator__C': [.001, .01, .1, 1, 10, 100, 1000],\n",
    "    }\n",
    "]\n",
    "\n",
    "model = StackingClassifier(\n",
    "    **{k.replace('model__', ''): v for k,v in best_estimators.items()},\n",
    "    final_estimator=LogisticRegression(\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Cross validation search\n",
    "search = cv_search(\n",
    "    pd.concat([X_train, X_test], axis=0, ignore_index=True),\n",
    "    pd.concat([y_train, y_test], axis=0, ignore_index=True),\n",
    "    Pipeline(steps=[\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    search_params,\n",
    "    cv=[(X_train.index, X_test.index + len(X_train))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, best_params = cv_results(search)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingClassifier(\n",
    "    **{k.replace('model__', ''): v for k,v in best_estimators.items()},\n",
    "    final_estimator=LogisticRegression(\n",
    "        **{k.replace('model__final_estimator__', ''): v for k,v in best_params.items()},\n",
    "        class_weight=weight_dict,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    cv=StratifiedKFold(5),\n",
    "    n_jobs=-1,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# Cross validation\n",
    "cv_dict = cross_validation(\n",
    "    Pipeline(steps=[\n",
    "        ('model', model),\n",
    "    ], verbose=True),\n",
    "    X_train,\n",
    "    y_train,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "train_test_evaluation(\n",
    "    'Model Stacking (Meta Tuned)',\n",
    "    metric_dict,\n",
    "    cv_dict,\n",
    "    model,\n",
    "    X_train, y_train, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(dict_sampling(metric_dict, [0, 2] + list(np.linspace(15, 18, 4))),\n",
    "           'Phase 10 - Box - Model Stacking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_matrix(dict_sampling(metric_dict, [0, 2] + list(np.linspace(15, 18, 4))),\n",
    "            'Phase 10 - Confusion Matrix - Model Stacking',\n",
    "            z_max=y_test.value_counts().values[-1],\n",
    "            max_col=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
